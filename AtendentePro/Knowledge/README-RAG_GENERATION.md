# ü§ñ RAG Generation - Sistema de Gera√ß√£o de Respostas

Este documento explica como o sistema RAG (Retrieval-Augmented Generation) gera respostas inteligentes baseadas nos documentos processados.

## üß† Arquitetura do Sistema RAG

```
Documentos PDF ‚Üí Extra√ß√£o de Texto ‚Üí Chunking ‚Üí Embeddings ‚Üí Busca Sem√¢ntica ‚Üí Gera√ß√£o de Resposta
```

## üìä Fluxo de Gera√ß√£o de Respostas

### 1. **Processamento de Documentos**
```python
# Extra√ß√£o de texto dos PDFs
documents = processor.process_documents()

# Cria√ß√£o de chunks sobrepostos
chunks = processor.create_chunks(chunk_size=1000, overlap=200)
```

### 2. **Gera√ß√£o de Embeddings**
```python
# Cada chunk √© convertido em vetor de embeddings
for chunk in chunks:
    embedding = client.embeddings.create(
        model="text-embedding-3-large",
        input=chunk['content']
    )
```

### 3. **Busca Sem√¢ntica**
```python
# Pergunta do usu√°rio √© convertida em embedding
query_embedding = client.embeddings.create(
    model="text-embedding-3-small", 
    input=user_question
)

# C√°lculo de similaridade com todos os chunks
similarities = cosine_similarity(query_embedding, chunk_embeddings)
```

### 4. **Gera√ß√£o de Resposta**
```python
# Contexto relevante √© enviado para GPT-4.1
response = client.chat.completions.create(
    model="gpt-4.1",
    messages=[
        {"role": "system", "content": "Voc√™ √© um especialista em soldagem..."},
        {"role": "user", "content": f"Pergunta: {question}\nContexto: {context}"}
    ]
)
```

## üîç Modelos de Embedding Utilizados

### **text-embedding-3-large** (Para Documentos)
- **Dimens√µes**: 3072
- **Uso**: Chunks dos documentos
- **Vantagem**: Maior precis√£o para conte√∫do complexo

### **text-embedding-3-small** (Para Consultas)
- **Dimens√µes**: 1536  
- **Uso**: Perguntas dos usu√°rios
- **Vantagem**: Maior velocidade e efici√™ncia

## üìà Algoritmo de Similaridade

```python
def find_relevant_chunks(query, top_k=5):
    # 1. Converter pergunta em embedding
    query_embedding = get_embedding(query)
    
    # 2. Calcular similaridade com todos os chunks
    similarities = []
    for chunk_data in chunk_embeddings:
        similarity = cosine_similarity([query_embedding], [chunk_data['embedding']])[0][0]
        similarities.append((similarity, chunk_data))
    
    # 3. Ordenar por relev√¢ncia e retornar top_k
    similarities.sort(key=lambda x: x[0], reverse=True)
    return [chunk_data for _, chunk_data in similarities[:top_k]]
```

## üéØ Estrat√©gias de Chunking

### **Chunking por Caracteres**
- **Tamanho**: 1000 caracteres
- **Sobreposi√ß√£o**: 200 caracteres
- **Filtro**: Chunks com menos de 100 caracteres s√£o descartados

### **Benef√≠cios da Sobreposi√ß√£o**
- ‚úÖ Mant√©m contexto entre chunks
- ‚úÖ Evita perda de informa√ß√µes nas bordas
- ‚úÖ Melhora a recupera√ß√£o de informa√ß√µes

## üíæ Sistema de Cache de Embeddings

### **Salvamento Autom√°tico**
```python
# Embeddings s√£o salvos em:
embedding_folder/embeddings.pkl
```

### **Carregamento Inteligente**
```python
# Sistema verifica se embeddings existem
if embedding_file.exists():
    agent.load_embeddings()  # Carrega do cache
else:
    agent.process_and_embed_documents()  # Processa novos
```

### **Vantagens do Cache**
- ‚ö° **Velocidade**: Evita reprocessamento
- üí∞ **Economia**: Reduz custos de API
- üîÑ **Efici√™ncia**: Atualiza√ß√µes incrementais

## üß™ Processo de Gera√ß√£o Detalhado

### **Input**: Pergunta do Usu√°rio
```
"Como funciona o processo de recebimento de notas fiscais?"
```

### **Step 1**: Embedding da Pergunta
```python
query_embedding = [0.1, -0.3, 0.8, ...]  # 1536 dimens√µes
```

### **Step 2**: Busca nos Chunks
```python
# Top 5 chunks mais relevantes:
chunks = [
    {"similarity": 0.89, "content": "Processo de recebimento...", "source": "documento1.pdf"},
    {"similarity": 0.85, "content": "Notas fiscais devem...", "source": "documento2.pdf"},
    # ... mais chunks
]
```

### **Step 3**: Constru√ß√£o do Contexto
```python
context = """
Document: documento1.pdf
Content: Processo de recebimento de notas fiscais...
---
Document: documento2.pdf  
Content: Notas fiscais devem ser...
"""
```

### **Step 4**: Prompt para GPT-4.1
```python
prompt = f"""
Baseado no contexto fornecido, responda a pergunta do usu√°rio.
Se a informa√ß√£o n√£o estiver dispon√≠vel, declare isso claramente.

Pergunta: {question}
Contexto: {context}
"""
```

### **Output**: Resposta Gerada
```python
response = {
    'answer': 'O processo de recebimento de notas fiscais...',
    'sources': ['documento1.pdf', 'documento2.pdf'],
    'confidence': 0.85,
    'context_used': 'Processo de recebimento...'
}
```

## üéõÔ∏è Par√¢metros Configur√°veis

### **Chunking**
```python
chunk_size = 1000      # Tamanho dos chunks
overlap = 200          # Sobreposi√ß√£o entre chunks
min_chunk_size = 100   # Tamanho m√≠nimo v√°lido
```

### **Busca**
```python
top_k = 5             # N√∫mero de chunks relevantes
similarity_threshold = 0.1  # Limiar de similaridade m√≠nima
```

### **Gera√ß√£o**
```python
model = "gpt-4.1"     # Modelo de gera√ß√£o
temperature = 0.7      # Criatividade da resposta
max_tokens = 1000     # Limite de tokens na resposta
```

## üìä M√©tricas de Qualidade

### **Similaridade de Embeddings**
- **Alta (>0.8)**: Informa√ß√£o muito relevante
- **M√©dia (0.5-0.8)**: Informa√ß√£o moderadamente relevante  
- **Baixa (<0.5)**: Informa√ß√£o pouco relevante

### **Cobertura de Documentos**
- **Fonte √∫nica**: Resposta baseada em 1 documento
- **M√∫ltiplas fontes**: Resposta baseada em v√°rios documentos
- **Sem fonte**: Informa√ß√£o n√£o encontrada nos documentos

## üîß Otimiza√ß√µes Implementadas

### **1. Embeddings Diferenciados**
- Documentos: `text-embedding-3-large` (maior precis√£o)
- Consultas: `text-embedding-3-small` (maior velocidade)

### **2. Cache Inteligente**
- Embeddings s√£o salvos ap√≥s primeira gera√ß√£o
- Carregamento autom√°tico em execu√ß√µes subsequentes
- Atualiza√ß√£o incremental quando documentos mudam

### **3. Chunking Otimizado**
- Sobreposi√ß√£o para manter contexto
- Filtro de chunks muito pequenos
- Preserva√ß√£o de estrutura do documento

### **4. Busca Eficiente**
- C√°lculo de similaridade vetorizado
- Ordena√ß√£o otimizada dos resultados
- Limita√ß√£o do n√∫mero de chunks processados

## üöÄ Performance e Escalabilidade

### **Benchmarks T√≠picos**
- **Processamento**: ~2-3 segundos por documento PDF
- **Embedding**: ~0.5 segundos por chunk
- **Busca**: ~0.1 segundos por consulta
- **Gera√ß√£o**: ~2-5 segundos por resposta

### **Limita√ß√µes Atuais**
- **Tamanho m√°ximo**: ~50 documentos por execu√ß√£o
- **Mem√≥ria**: ~2GB para 1000 chunks
- **API Rate Limits**: 60 requests/minuto (OpenAI)

## üîÆ Melhorias Futuras

### **1. Embeddings Locais**
- Implementa√ß√£o de modelos de embedding locais
- Redu√ß√£o de depend√™ncia da API OpenAI
- Maior controle sobre os embeddings

### **2. Busca H√≠brida**
- Combina√ß√£o de busca sem√¢ntica e textual
- Melhoria na precis√£o da recupera√ß√£o
- Suporte a consultas mais complexas

### **3. Cache Distribu√≠do**
- Sistema de cache compartilhado
- Otimiza√ß√£o para m√∫ltiplos usu√°rios
- Redu√ß√£o de reprocessamento

### **4. Fine-tuning**
- Treinamento espec√≠fico para dom√≠nio
- Melhoria na qualidade das respostas
- Adapta√ß√£o ao vocabul√°rio t√©cnico

---

**Sistema RAG desenvolvido para AtendentePRO** ü§ñ‚ú®
